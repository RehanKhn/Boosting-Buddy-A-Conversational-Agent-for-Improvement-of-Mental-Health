{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TextGeneration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMCBKYEC0CJDp4xmCSFGklx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RehanKhn/Boosting-Buddy-A-Conversational-Agent-for-Improvement-of-Mental-Health/blob/main/TextGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7KETShauKvQ"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers.embeddings import Embedding\n",
        "import string\n",
        "import re\n",
        "import numpy\n",
        "import sys\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGmBDrKnufyf"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VGWNQ30ujlF"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':'1mrhXtn3Dq_XXZ4fW_GlSOn1upXWzNsH4'})\n",
        "downloaded.GetContentFile('TestRU.txt')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTHjKlgqwuej"
      },
      "source": [
        "file = open(\"TestRU.txt\").read()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEhhpyJCw12P"
      },
      "source": [
        "def tokenize_words(input):\n",
        "    input = input.lower()\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(input)\n",
        "    stopwords=[]\n",
        "    filtered = filter(lambda token: token not in stopwords, tokens)\n",
        "    return \" \".join(filtered)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMZtvwkdw8gd"
      },
      "source": [
        "processed_inputs = tokenize_words(file)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6rPzOZQxAUf"
      },
      "source": [
        "chars = sorted(list(set(processed_inputs)))\n",
        "char_to_num = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-cctBZbxGk4",
        "outputId": "b402f0fd-fa21-4abb-a061-705e71b8947d"
      },
      "source": [
        "input_len = len(processed_inputs)\n",
        "vocab_len = len(chars)\n",
        "print (\"Total number of characters:\", input_len)\n",
        "print (\"Total vocab:\", vocab_len)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of characters: 20017\n",
            "Total vocab: 29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_PZxns6xI6l"
      },
      "source": [
        "seq_length = 100\n",
        "x_data = []\n",
        "y_data = []"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0dgGmoqxLKa"
      },
      "source": [
        "for i in range(0, input_len - seq_length, 1):\n",
        "    in_seq = processed_inputs[i:i + seq_length]\n",
        "    out_seq = processed_inputs[i + seq_length]\n",
        "    x_data.append([char_to_num[char] for char in in_seq])\n",
        "    y_data.append(char_to_num[out_seq])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mhqt42erxOwt",
        "outputId": "7b689b47-2a64-4397-f6a5-404e6a0a8b6a"
      },
      "source": [
        "n_patterns = len(x_data)\n",
        "print (\"Total Patterns:\", n_patterns)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns: 19917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPf4I2huxRgT"
      },
      "source": [
        "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
        "X = X/float(vocab_len)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4trKlMZxTz1"
      },
      "source": [
        "y = np_utils.to_categorical(y_data)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjgeh86ixXt1"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256,input_shape=(X.shape[1],X.shape[2]),return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nHuuqE_xZ_C"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_OTD_T1xhjm"
      },
      "source": [
        "filepath = \"model_weights_saved.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "desired_callbacks = [checkpoint]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qofa9X3WxlP0",
        "outputId": "707bcc89-aec4-49b3-f23d-df925ed173d8"
      },
      "source": [
        "model.fit(X, y, epochs=20, batch_size=128, callbacks=desired_callbacks)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "78/78 [==============================] - 36s 53ms/step - loss: 2.9522\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.87046, saving model to model_weights_saved.hdf5\n",
            "Epoch 2/20\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 2.8345\n",
            "\n",
            "Epoch 00002: loss improved from 2.87046 to 2.82707, saving model to model_weights_saved.hdf5\n",
            "Epoch 3/20\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 2.8307\n",
            "\n",
            "Epoch 00003: loss improved from 2.82707 to 2.82148, saving model to model_weights_saved.hdf5\n",
            "Epoch 4/20\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 2.8093\n",
            "\n",
            "Epoch 00004: loss improved from 2.82148 to 2.81848, saving model to model_weights_saved.hdf5\n",
            "Epoch 5/20\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 2.8150\n",
            "\n",
            "Epoch 00005: loss improved from 2.81848 to 2.80800, saving model to model_weights_saved.hdf5\n",
            "Epoch 6/20\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 2.7668\n",
            "\n",
            "Epoch 00006: loss improved from 2.80800 to 2.75418, saving model to model_weights_saved.hdf5\n",
            "Epoch 7/20\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 2.6527\n",
            "\n",
            "Epoch 00007: loss improved from 2.75418 to 2.62809, saving model to model_weights_saved.hdf5\n",
            "Epoch 8/20\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 2.5242\n",
            "\n",
            "Epoch 00008: loss improved from 2.62809 to 2.50653, saving model to model_weights_saved.hdf5\n",
            "Epoch 9/20\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 2.4240\n",
            "\n",
            "Epoch 00009: loss improved from 2.50653 to 2.40394, saving model to model_weights_saved.hdf5\n",
            "Epoch 10/20\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 2.3440\n",
            "\n",
            "Epoch 00010: loss improved from 2.40394 to 2.31989, saving model to model_weights_saved.hdf5\n",
            "Epoch 11/20\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 2.2560\n",
            "\n",
            "Epoch 00011: loss improved from 2.31989 to 2.23398, saving model to model_weights_saved.hdf5\n",
            "Epoch 12/20\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 2.1549\n",
            "\n",
            "Epoch 00012: loss improved from 2.23398 to 2.14938, saving model to model_weights_saved.hdf5\n",
            "Epoch 13/20\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 2.0929\n",
            "\n",
            "Epoch 00013: loss improved from 2.14938 to 2.07891, saving model to model_weights_saved.hdf5\n",
            "Epoch 14/20\n",
            "78/78 [==============================] - 4s 53ms/step - loss: 2.0139\n",
            "\n",
            "Epoch 00014: loss improved from 2.07891 to 2.00244, saving model to model_weights_saved.hdf5\n",
            "Epoch 15/20\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 1.9241\n",
            "\n",
            "Epoch 00015: loss improved from 2.00244 to 1.92432, saving model to model_weights_saved.hdf5\n",
            "Epoch 16/20\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 1.8662\n",
            "\n",
            "Epoch 00016: loss improved from 1.92432 to 1.85436, saving model to model_weights_saved.hdf5\n",
            "Epoch 17/20\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 1.7740\n",
            "\n",
            "Epoch 00017: loss improved from 1.85436 to 1.77815, saving model to model_weights_saved.hdf5\n",
            "Epoch 18/20\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 1.7074\n",
            "\n",
            "Epoch 00018: loss improved from 1.77815 to 1.70817, saving model to model_weights_saved.hdf5\n",
            "Epoch 19/20\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 1.6530\n",
            "\n",
            "Epoch 00019: loss improved from 1.70817 to 1.64708, saving model to model_weights_saved.hdf5\n",
            "Epoch 20/20\n",
            "78/78 [==============================] - 4s 54ms/step - loss: 1.5591\n",
            "\n",
            "Epoch 00020: loss improved from 1.64708 to 1.56631, saving model to model_weights_saved.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9f9023c710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RQ1dzx4SOA6",
        "outputId": "6be0cb6b-d90d-4c48-d2f6-de3765f66861"
      },
      "source": [
        "train_acc = model.evaluate(X, y, verbose=1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "623/623 [==============================] - 7s 9ms/step - loss: 1.3904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRbZ8OlMxy1I"
      },
      "source": [
        "filename = \"model_weights_saved.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8hgMbr5x0gy"
      },
      "source": [
        "num_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqLsHIEqx05E",
        "outputId": "9009e821-6a1f-450a-fe88-0d8de55c34a3"
      },
      "source": [
        "start = numpy.random.randint(0, len(x_data) - 1)\n",
        "pattern = x_data[start]\n",
        "print(\"Random Seed:\")\n",
        "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:\n",
            "\"  tha aur mjhy sab kuch khud seekhna tha anger meri summer job mein meri kesi ny madad nahi ki aur mu \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmEr4o0ax1Lz",
        "outputId": "8bae1560-c7f0-45ba-a670-0f619146fce6"
      },
      "source": [
        "for i in range(1000):\n",
        "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(vocab_len)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = numpy.argmax(prediction)\n",
        "    result = num_to_char[index]\n",
        "    seq_in = [num_to_char[value] for value in pattern]\n",
        "\n",
        "    sys.stdout.write(result)\n",
        "\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jhe pars kr lila jis me me ne acha naii ka ha tha fear me pe kari sa mara tha fear me pe kisi sa milna hona joy jb mere bacha tha joy jb mere bacha tha joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb mere bacha paida hoa joy jb m"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s48w3MFJwX83"
      },
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}